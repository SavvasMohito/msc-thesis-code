{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Unified Security Requirements Crew\n",
        "\n",
        "This notebook allows you to test and debug the unified crew architecture step-by-step.\n",
        "\n",
        "**Setup:**\n",
        "1. Make sure your `.env` file has OPENAI_API_KEY\n",
        "2. Ensure Weaviate is running (for map_security_controls task)\n",
        "3. Run cells in order (or jump to specific task)\n",
        "\n",
        "**Features:**\n",
        "- Test individual tasks or full crew\n",
        "- Cache intermediate results\n",
        "- Debug specific tasks (especially map_security_controls)\n",
        "- Resume from any point\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì OpenAI API key loaded\n",
            "‚úì Setup complete\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(Path.cwd() / \"src\"))\n",
        "\n",
        "# Load environment\n",
        "load_dotenv()\n",
        "\n",
        "# Verify API key is loaded\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\"‚ö†Ô∏è  WARNING: OPENAI_API_KEY not found in environment!\")\n",
        "    print(\"   Please add it to your .env file\")\n",
        "else:\n",
        "    print(\"‚úì OpenAI API key loaded\")\n",
        "\n",
        "print(\"‚úì Setup complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking Weaviate connection...\n",
            "‚úì Weaviate is connected and ready\n",
            "‚úì SecurityControl collection exists\n",
            "‚úì Test query successful (found 1 results)\n"
          ]
        }
      ],
      "source": [
        "# Weaviate Configuration and Health Check\n",
        "import weaviate\n",
        "\n",
        "# Set Weaviate connection parameters\n",
        "os.environ.setdefault(\"WEAVIATE_HOST\", \"localhost\")\n",
        "os.environ.setdefault(\"WEAVIATE_PORT\", \"8080\")\n",
        "os.environ.setdefault(\"WEAVIATE_GRPC_PORT\", \"50051\")\n",
        "\n",
        "print(\"Checking Weaviate connection...\")\n",
        "try:\n",
        "    client = weaviate.connect_to_local(\n",
        "        host=os.getenv(\"WEAVIATE_HOST\"),\n",
        "        port=int(os.getenv(\"WEAVIATE_PORT\")),\n",
        "        grpc_port=int(os.getenv(\"WEAVIATE_GRPC_PORT\")),\n",
        "    )\n",
        "\n",
        "    # Check if connected\n",
        "    if client.is_ready():\n",
        "        print(\"‚úì Weaviate is connected and ready\")\n",
        "\n",
        "        # Check if SecurityControl collection exists\n",
        "        if client.collections.exists(\"SecurityControl\"):\n",
        "            collection = client.collections.get(\"SecurityControl\")\n",
        "            print(\"‚úì SecurityControl collection exists\")\n",
        "\n",
        "            # Try a simple query to verify it works\n",
        "            try:\n",
        "                response = collection.query.near_text(query=\"authentication\", limit=1)\n",
        "                print(f\"‚úì Test query successful (found {len(response.objects)} results)\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Test query failed: {e}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  WARNING: SecurityControl collection does not exist!\")\n",
        "            print(\"   Run: python -m security_requirements_system.tools.weaviate_setup\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  WARNING: Weaviate is not ready\")\n",
        "\n",
        "    client.close()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR: Cannot connect to Weaviate: {e}\")\n",
        "    print(f\"   Make sure Weaviate is running on {os.getenv('WEAVIATE_HOST')}:{os.getenv('WEAVIATE_PORT')}\")\n",
        "    print(\"   Start with: docker-compose up -d\")\n",
        "    print(\"\\nWeaviate Configuration:\")\n",
        "    print(f\"  Host: {os.getenv('WEAVIATE_HOST')}\")\n",
        "    print(f\"  Port: {os.getenv('WEAVIATE_PORT')}\")\n",
        "    print(f\"  gRPC Port: {os.getenv('WEAVIATE_GRPC_PORT')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Helper functions loaded\n",
            "  Project root: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system\n",
            "  Test outputs dir: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/test_outputs/unified_crew\n",
            "  Test outputs exists: True\n"
          ]
        }
      ],
      "source": [
        "# Helper functions\n",
        "\n",
        "# Determine project root directory\n",
        "_current_dir = Path.cwd()\n",
        "PROJECT_ROOT = _current_dir\n",
        "\n",
        "# Look for project root indicators\n",
        "max_depth = 5\n",
        "for depth in range(max_depth):\n",
        "    if (PROJECT_ROOT / \"test_outputs\").exists() or (PROJECT_ROOT / \"pyproject.toml\").exists() or (PROJECT_ROOT / \"src\").exists():\n",
        "        break\n",
        "    parent = PROJECT_ROOT.parent\n",
        "    if parent == PROJECT_ROOT:\n",
        "        break\n",
        "    PROJECT_ROOT = parent\n",
        "else:\n",
        "    if \"tests\" in str(_current_dir):\n",
        "        PROJECT_ROOT = _current_dir.parent\n",
        "\n",
        "TEST_OUTPUTS_DIR = PROJECT_ROOT / \"test_outputs\" / \"unified_crew\"\n",
        "TEST_OUTPUTS_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "\n",
        "def save_task_output(task_name, result, output_dir=None):\n",
        "    \"\"\"Save task output to JSON file.\"\"\"\n",
        "    if output_dir is None:\n",
        "        output_path = TEST_OUTPUTS_DIR\n",
        "    else:\n",
        "        output_path = Path(output_dir)\n",
        "\n",
        "    output_path.mkdir(exist_ok=True, parents=True)\n",
        "    output_file = output_path / f\"{task_name}_output.json\"\n",
        "\n",
        "    data = {\n",
        "        \"task_name\": task_name,\n",
        "        \"raw\": result.raw if hasattr(result, \"raw\") else str(result),\n",
        "    }\n",
        "\n",
        "    if hasattr(result, \"pydantic\") and result.pydantic:\n",
        "        data[\"pydantic\"] = result.pydantic.model_dump()\n",
        "\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump(data, f, indent=2, default=str)\n",
        "\n",
        "    print(f\"\\nüíæ Saved to: {output_file}\")\n",
        "    return output_file\n",
        "\n",
        "\n",
        "def load_task_output(task_name, output_dir=None):\n",
        "    \"\"\"Load cached task output from JSON file if it exists.\"\"\"\n",
        "    if output_dir is None:\n",
        "        output_path = TEST_OUTPUTS_DIR\n",
        "    else:\n",
        "        output_path = Path(output_dir)\n",
        "\n",
        "    output_file = output_path / f\"{task_name}_output.json\"\n",
        "\n",
        "    if output_file.exists():\n",
        "        with open(output_file, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "        print(f\"üìÇ Loaded cached output from: {output_file}\")\n",
        "        return data\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def display_task_output(result, task_name):\n",
        "    \"\"\"Display task output in a readable format.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{task_name} OUTPUT\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    if hasattr(result, \"raw\"):\n",
        "        print(\"RAW OUTPUT (first 500 chars):\")\n",
        "        print(result.raw[:500] + \"...\" if len(result.raw) > 500 else result.raw)\n",
        "\n",
        "    if hasattr(result, \"pydantic\") and result.pydantic:\n",
        "        print(\"\\nSTRUCTURED OUTPUT:\")\n",
        "        print(json.dumps(result.pydantic.model_dump(), indent=2, default=str)[:1000])\n",
        "\n",
        "\n",
        "print(\"‚úì Helper functions loaded\")\n",
        "print(f\"  Project root: {PROJECT_ROOT}\")\n",
        "print(f\"  Test outputs dir: {TEST_OUTPUTS_DIR}\")\n",
        "print(f\"  Test outputs exists: {TEST_OUTPUTS_DIR.exists()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Sample requirements loaded\n",
            "Length: 661 characters\n"
          ]
        }
      ],
      "source": [
        "# Sample requirements text\n",
        "\n",
        "SAMPLE_REQUIREMENTS = \"\"\"\n",
        "Task Management System Requirements:\n",
        "\n",
        "1. User Management\n",
        "   - User registration and login\n",
        "   - Multi-factor authentication\n",
        "   - Password reset functionality\n",
        "   - Profile management\n",
        "\n",
        "2. Task Management\n",
        "   - Create, edit, and delete tasks\n",
        "   - Assign tasks to users\n",
        "   - Set task priorities and deadlines\n",
        "   - Task status tracking (todo, in-progress, done)\n",
        "\n",
        "3. Project Management\n",
        "   - Create and manage projects\n",
        "   - Assign team members to projects\n",
        "   - Project-level permissions\n",
        "\n",
        "4. Reporting\n",
        "   - Task completion reports\n",
        "   - User activity reports\n",
        "   - Project progress dashboards\n",
        "\n",
        "5. Admin Panel\n",
        "   - User management\n",
        "   - System configuration\n",
        "   - Audit logs\n",
        "\"\"\"\n",
        "\n",
        "print(\"‚úì Sample requirements loaded\")\n",
        "print(f\"Length: {len(SAMPLE_REQUIREMENTS)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Test Individual Tasks\n",
        "\n",
        "Test each task individually to debug issues. Tasks are executed sequentially, so each task can access outputs from previous tasks via context.\n",
        "\n",
        "### Task 5: Map Security Controls (DEBUG FOCUS)\n",
        "\n",
        "**This is where the code froze!** Let's test this task specifically with detailed debugging.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from security_requirements_system.crew import SecurityRequirementsCrew\n",
        "from security_requirements_system.data_models import DomainSecurityOutput, AnalysisOutput\n",
        "from security_requirements_system.tools.weaviate_tool import WeaviateQueryTool\n",
        "\n",
        "print(\"Testing Task 5: Map Security Controls (DEBUG MODE)...\\n\")\n",
        "\n",
        "# Load previous outputs if not in memory\n",
        "if \"analysis_output\" not in globals():\n",
        "    cached = load_task_output(\"analyze_requirements\")\n",
        "    if cached and \"pydantic\" in cached:\n",
        "        analysis_output = AnalysisOutput(**cached[\"pydantic\"])\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Loading analysis_output from full crew run...\")\n",
        "        # Run first task to get analysis_output\n",
        "        crew_instance = SecurityRequirementsCrew()\n",
        "        crew = crew_instance.crew()\n",
        "        result = crew.kickoff(inputs={\"requirements_text\": SAMPLE_REQUIREMENTS})\n",
        "        if result.tasks_output:\n",
        "            analysis_output = result.tasks_output[0].pydantic\n",
        "\n",
        "# First, test the WeaviateQueryTool directly\n",
        "print(\"\\n1. Testing WeaviateQueryTool directly...\")\n",
        "try:\n",
        "    tool = WeaviateQueryTool()\n",
        "    test_result = tool._run(query=\"authentication\", limit=3)\n",
        "    print(f\"‚úì Tool test successful\")\n",
        "    print(f\"   Result preview: {test_result[:200]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Tool test failed: {e}\")\n",
        "    import traceback\n",
        "\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Check for cached output\n",
        "cached = load_task_output(\"map_security_controls\")\n",
        "if cached:\n",
        "    print(\"\\n‚ö†Ô∏è  Found cached output. Delete the cache file to re-run.\")\n",
        "    if \"pydantic\" in cached:\n",
        "        domain_output = DomainSecurityOutput(**cached[\"pydantic\"])\n",
        "        print(f\"‚úì Loaded cached output\")\n",
        "else:\n",
        "    print(\"\\n2. Running map_security_controls task...\")\n",
        "    print(\"   This may take a while as it queries Weaviate for each requirement...\")\n",
        "\n",
        "    # Run crew - it will use context from previous tasks\n",
        "    crew_instance = SecurityRequirementsCrew()\n",
        "    crew = crew_instance.crew()\n",
        "\n",
        "    try:\n",
        "        result = crew.kickoff(inputs={\"requirements_text\": SAMPLE_REQUIREMENTS})\n",
        "\n",
        "        # Extract fifth task output\n",
        "        if len(result.tasks_output) >= 5:\n",
        "            task_result = result.tasks_output[4]  # Fifth task\n",
        "            display_task_output(task_result, \"Map Security Controls\")\n",
        "            save_task_output(\"map_security_controls\", task_result)\n",
        "\n",
        "            if task_result.pydantic:\n",
        "                domain_output = task_result.pydantic\n",
        "                print(f\"\\n‚úì Requirements mapped: {len(domain_output.requirements_mapping) if domain_output.requirements_mapping else 0}\")\n",
        "\n",
        "                # Count controls\n",
        "                total_controls = 0\n",
        "                if domain_output.requirements_mapping:\n",
        "                    for rm in domain_output.requirements_mapping:\n",
        "                        total_controls += len(rm.security_controls) if rm.security_controls else 0\n",
        "                print(f\"‚úì Total controls mapped: {total_controls}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Expected 5 tasks, got {len(result.tasks_output)}\")\n",
        "            print(f\"   Completed tasks: {[t.name for t in result.tasks_output]}\")\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n‚ö†Ô∏è  Task interrupted by user (KeyboardInterrupt)\")\n",
        "        print(\"   This suggests the task is taking too long or hanging\")\n",
        "        print(\"   Check Weaviate connection and tool configuration\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error during task execution: {e}\")\n",
        "        import traceback\n",
        "\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Test Full Crew Execution\n",
        "\n",
        "Run the entire crew from start to finish.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test full crew execution\n",
        "print(\"Testing Full Crew Execution...\\n\")\n",
        "print(\"This will run all 9 tasks sequentially.\\n\")\n",
        "\n",
        "crew_instance = SecurityRequirementsCrew()\n",
        "crew = crew_instance.crew()\n",
        "\n",
        "try:\n",
        "    result = crew.kickoff(inputs={\"requirements_text\": SAMPLE_REQUIREMENTS})\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"CREW EXECUTION COMPLETE\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    print(f\"Total tasks completed: {len(result.tasks_output)}\\n\")\n",
        "\n",
        "    task_names = [\n",
        "        \"analyze_requirements\",\n",
        "        \"analyze_architecture\",\n",
        "        \"analyze_stakeholders_and_compliance\",\n",
        "        \"perform_threat_modeling\",\n",
        "        \"map_security_controls\",\n",
        "        \"identify_ai_security_requirements\",\n",
        "        \"design_security_architecture\",\n",
        "        \"create_implementation_and_testing_plan\",\n",
        "        \"validate_security_requirements\",\n",
        "    ]\n",
        "\n",
        "    for i, task in enumerate(result.tasks_output):\n",
        "        task_name = task.name if hasattr(task, \"name\") else task_names[i] if i < len(task_names) else f\"Task {i+1}\"\n",
        "        status = \"‚úì\" if hasattr(task, \"pydantic\") and task.pydantic else \"‚ö†\"\n",
        "        print(f\"{status} {i+1}. {task_name}\")\n",
        "\n",
        "        # Save each task output\n",
        "        if i < len(task_names):\n",
        "            save_task_output(task_names[i], task)\n",
        "\n",
        "    # Show final validation\n",
        "    if len(result.tasks_output) >= 9:\n",
        "        validation_task = result.tasks_output[8]\n",
        "        if validation_task.pydantic:\n",
        "            from security_requirements_system.data_models import ValidationOutput\n",
        "\n",
        "            validation_output = validation_task.pydantic\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"FINAL VALIDATION\")\n",
        "            print(f\"{'='*60}\")\n",
        "            print(f\"Overall Score: {validation_output.overall_score:.2f}\")\n",
        "            print(f\"Validation Passed: {validation_output.validation_passed}\")\n",
        "            print(f\"\\nDimension Scores:\")\n",
        "            for dim, score in validation_output.dimension_scores.items():\n",
        "                print(f\"  - {dim}: {score:.2f}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚ö†Ô∏è  Execution interrupted by user\")\n",
        "    print(\"   Check which task was running when interrupted\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")\n",
        "    import traceback\n",
        "\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test WeaviateQueryTool with different queries\n",
        "print(\"Testing WeaviateQueryTool with various queries...\\n\")\n",
        "\n",
        "from security_requirements_system.tools.weaviate_tool import WeaviateQueryTool\n",
        "\n",
        "tool = WeaviateQueryTool()\n",
        "\n",
        "test_queries = [\n",
        "    (\"authentication\", 5),\n",
        "    (\"encryption\", 5),\n",
        "    (\"access control\", 5),\n",
        "    (\"input validation\", 3),\n",
        "]\n",
        "\n",
        "for query, limit in test_queries:\n",
        "    try:\n",
        "        print(f\"\\nQuery: '{query}' (limit={limit})\")\n",
        "        result = tool._run(query=query, limit=limit)\n",
        "        print(f\"‚úì Success: {len(result)} characters\")\n",
        "        print(f\"   Preview: {result[:150]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed: {e}\")\n",
        "        import traceback\n",
        "\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all cached outputs\n",
        "print(\"Cached Task Outputs:\\n\")\n",
        "\n",
        "cached_files = list(TEST_OUTPUTS_DIR.glob(\"*_output.json\"))\n",
        "if cached_files:\n",
        "    for file in sorted(cached_files):\n",
        "        size = file.stat().st_size / 1024  # KB\n",
        "        print(f\"  - {file.name} ({size:.2f} KB)\")\n",
        "else:\n",
        "    print(\"  No cached outputs found\")\n",
        "    print(f\"  Output directory: {TEST_OUTPUTS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect a specific cached task output\n",
        "task_to_inspect = \"map_security_controls\"  # Change this to inspect different tasks\n",
        "\n",
        "cached = load_task_output(task_to_inspect)\n",
        "if cached:\n",
        "    print(f\"Inspecting {task_to_inspect}:\\n\")\n",
        "    print(f\"Raw output length: {len(cached.get('raw', ''))} characters\")\n",
        "\n",
        "    if \"pydantic\" in cached:\n",
        "        print(f\"\\nStructured output keys: {list(cached['pydantic'].keys())}\")\n",
        "\n",
        "        # Pretty print the structured output\n",
        "        print(\"\\nStructured Output:\")\n",
        "        print(json.dumps(cached[\"pydantic\"], indent=2, default=str)[:2000])\n",
        "    else:\n",
        "        print(\"\\nRaw Output (first 1000 chars):\")\n",
        "        print(cached.get(\"raw\", \"\")[:1000])\n",
        "else:\n",
        "    print(f\"No cached output found for {task_to_inspect}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
